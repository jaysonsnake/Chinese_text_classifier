{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 用机器学习方法完成中文文本分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 文本数据特征工程方法\n",
    "\n",
    "- BOW/词袋模型\n",
    "- TF-IDF\n",
    "- word2vec\n",
    "\n",
    "### 文本分类模型\n",
    "\n",
    "- NB/SVM/GBDT\n",
    "- Fasttext\n",
    "- CNN/LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 朴素贝叶斯"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们试试用朴素贝叶斯完成一个中文文本分类器，一般在数据量足够，数据丰富度够的情况下，用朴素贝叶斯完成这个任务，准确度还是不错的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "机器学习的算法要取得好效果，离不开数据，我们先把数据加载进来看看。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 准备数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "准备好数据，我们挑选 科技、汽车、娱乐、军事、运动 总共5类文本数据进行处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import pandas as pd\n",
    "\n",
    "# 科技\n",
    "df_technology = pd.read_csv(\"./data/technology_news.csv\", encoding='utf-8')\n",
    "df_technology = df_technology.dropna()\n",
    "\n",
    "# 汽车\n",
    "df_car = pd.read_csv(\"./data/car_news.csv\", encoding='utf-8')\n",
    "df_car = df_car.dropna()\n",
    "\n",
    "# 娱乐\n",
    "df_entertainment = pd.read_csv(\"./data/entertainment_news.csv\", encoding='utf-8')\n",
    "df_entertainment = df_entertainment.dropna()\n",
    "\n",
    "# 军事\n",
    "df_military = pd.read_csv(\"./data/military_news.csv\", encoding='utf-8')\n",
    "df_military = df_military.dropna()\n",
    "\n",
    "# 体育\n",
    "df_sports = pd.read_csv(\"./data/sports_news.csv\", encoding='utf-8')\n",
    "df_sports = df_sports.dropna()\n",
    "\n",
    "technology = df_technology.content.values.tolist()[1000:21000]\n",
    "car = df_car.content.values.tolist()[1000:21000]\n",
    "entertainment = df_entertainment.content.values.tolist()[:20000]\n",
    "military = df_military.content.values.tolist()[:20000]\n",
    "sports = df_sports.content.values.tolist()[:20000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "随便挑几条数据看看"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "　　现在家里都拉了网线，都能无线上网，一定要帮他们先登上WiFi，另外，老人不懂得流量是什么，也不知道如何开关，控制流量，所以设置好流量上限很重要，免得不小心点开了视频或者下载，电话费就大发了。\n"
     ]
    }
   ],
   "source": [
    "print(technology[12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "　　截至发稿时，人人车给出的处理方案仍旧是检修车辆。王先生则认为，车辆在购买时就存在问题，但交易平台并未能检测出来。因此，王先生希望对方退款。王先生称，他将找专业机构对车辆进行鉴定，并通过法律途径维护自己的权益。J256\n"
     ]
    }
   ],
   "source": [
    "print(car[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "　　网综尺度相对较大原本是制作优势，《奇葩说》也曾经因为讨论的话题较为前卫一度引发争议。但《奇葩说》对于价值观的把握和引导让其中内含的“少儿不宜”只能算是小花絮。而纯粹是为了制造话题而“污”得“无节操无下限”的网综不仅让人生厌，也给节目自身招致了下架的厄运。对资本方而言，即便只从商业运营考量，点击量也分有价值和无价值，节目内容的变现能力如果建立在博眼球和低趣味迎合上，商业运营也难长久。对节目制作方与平台来说，为博一时的高点击而不顾底线不仅是砸自己招牌，以噱头吸引而来的观众与流量也是难以维持。\n"
     ]
    }
   ],
   "source": [
    "print(entertainment[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "　　央视记者 胡善敏：我现在所处的位置是在辽宁舰的飞行甲板，执行跨海区训练和试验任务的辽宁舰官兵，正在展开多个科目的训练，穿着不同颜色服装的官兵在紧张的对舰载机进行转运。\n"
     ]
    }
   ],
   "source": [
    "print(military[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "　　据统计，2016年仅在中国田径协会注册的马拉松赛事便达到了328场，继续呈现出爆发式增长的态势，2015年，这个数字还仅仅停留在134场。如果算上未在中国田协注册的纯“民间”赛事，国内全年的路跑赛事还要更多。\n"
     ]
    }
   ],
   "source": [
    "print(sports[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分词与中文文本处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 停用词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = pd.read_csv(\"data/stopwords.txt\",\n",
    "                        index_col = False,\n",
    "                        quoting = 3,\n",
    "                        sep = \"\\t\",\n",
    "                        names = ['stopword'],\n",
    "                        encoding = 'utf-8'\n",
    "                       )\n",
    "stopwords = stopwords['stopword'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 去停用词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 1.425 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "def preprocess_text(content_lines, sentences, category):\n",
    "    for line in content_lines:\n",
    "        try:\n",
    "            segs = jieba.lcut(line) # jieba分词，存入一个list中\n",
    "            segs = filter(lambda x:len(x)>1, segs) # 只保留长度大于1的词\n",
    "            segs = filter(lambda x:x not in stopwords, segs) # 只保留非停用词\n",
    "            sentences.append((\" \".join(segs), category)) # 用空格拼接过滤好的词，并在每个词后写上类别category\n",
    "        except Exception as e:\n",
    "            print(line)\n",
    "            continue\n",
    "            \n",
    "# 生成训练数据\n",
    "sentences = []\n",
    "\n",
    "preprocess_text(technology, sentences, 'technology')\n",
    "preprocess_text(car, sentences, 'car')\n",
    "preprocess_text(entertainment, sentences, 'entertainment')\n",
    "preprocess_text(military, sentences, 'military')\n",
    "preprocess_text(sports, sentences, 'sports')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 生成训练集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们打乱一下顺序，生成更可靠的训练集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "苏宁 青创园 专注 青年 电商 文化 创意 影视制作 动漫 二次元 互联网 消费 升级 领域 项目 孵化 创业者 提供 完备 创业 环境 园区 一期 项目 占地约 9000 平方米 包含 四栋 众创 空间 两栋 商业 配套 录音棚 摄影棚 直播间 专业 配套 设施 基础 环境 服务 资源 生态 支持 青创园 背靠 苏宁 六大 产业 平台 平台 背后 更是 汇聚 用户 十万家 合作伙伴 完善 产业 服务 生态圈 technology\n",
      "当晚 建安 飞机 顾不上 车旅 劳顿 直奔 医院 详细 患者 病情 第二天 一早 医生 交流 提出 治疗 建议 讨论 修改 治疗 方案 鼓励 安慰 患者 离开 医院 建安 许诺 电话 随叫随到 这名 边防 干部 康复 出院 military\n",
      "人物 不知 拥护 爱戴 崇仰 国家 希望 奴隶 之邦 这句 名言 一代人 牢牢记 military\n",
      "未来 中车 制造 停靠 波士顿 久负盛名 哈佛大学 麻省理工学院 走过 密歇根湖 芝加哥 抵达 洛杉矶 影星 荟萃 好莱坞 穿行 费城 世界 发达 市场 通向 世界 窗口 城市 展示 中国 风采 car\n",
      "马化腾 去年 腾讯 电信 网络 诈骗 联合 大会 企业 个人信息 保护 视为 一项 工作 腾讯 重视 用户 信息安全 用户 个人信息 谨慎 采集 妥善 运营 保护 用户 知情权 数据 设置 防护 标准 专业 团队 采用 纵深 防御 理念 网络 主机 层面 相关 加固 措施 确保 用户 个人信息 马化腾 建议 提出 希望 政府 主管部门 牵头 行业 构建 个人信息 分级分类 保护 体系 完善 相关 岗位 工作人员 规范 管理 监督 technology\n",
      "采写 京报 实习生 邵程 发自 乌镇 entertainment\n",
      "本剧 不靠 天价 大腕 吸引 眼球 大胆 启用 一批 充满活力 年轻 演员 担任 主角 王海燕 冯国强 老戏骨 坐镇 甘当 绿叶 entertainment\n",
      "X70 搭载 最强 智能 电视 旗舰 芯片 Mstar6A938 1.7 GHz 处理器 32GB 存储配置 性能 强劲 需求 系统 搭载 最新 EUI 5.9 拥有 分众 运营 个性 推荐 超级 语音 智能 导视 功能 蓝牙 4.1 802.11 ac 双频 Wi Fi USB3.0 接口 用户 连接 设备 音响 搭载 哈曼 卡顿 专家 团队 设计 审核 专业 认证 音响系统 搭配 杜比 音效 Dolby Audio 解码 后处理 技术 用户 提供 清晰 震撼人心 音效 看电视 娱乐 享受 标配 支持 DTS 解码 technology\n",
      "面向未来 广汽所 关注 汽车 用户 需求 出发点 开拓创新 高品质 汽车 产品 基础 消费者 提供 定制 出行 方案 消费者 创造 美好 生活 体验 力争 十三 期末 广汽 品牌 中国 汽车行业 名优 品牌 全球化 影响力 具备 高度 社会 责任感 国际 汽车集团 品牌 car\n",
      "维权 出击 car\n"
     ]
    }
   ],
   "source": [
    "# 输出一下前10个句子\n",
    "for sentence in sentences[:10]:\n",
    "    print(sentence[0], sentence[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了一会儿检测一下咱们的分类器效果怎么样，我们需要一份测试集。\n",
    "\n",
    "所以把原数据集分成训练集的测试集，咱们用sklearn自带的分割函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x, y = zip(*sentences)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65696"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下一步要做的就是在降噪数据上抽取出有用的特征啦，我们对文本抽取词袋模型特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vec = CountVectorizer(\n",
    "    analyzer='word', # tokenise by character ngrams 解析器：基于空格来区分不同的词\n",
    "    max_features=4000, # keep the most common 4000 ngrams 保留最高频的4000个词\n",
    ")\n",
    "vec.fit(x_train)\n",
    "\n",
    "def get_features(x):\n",
    "    vec.transform(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "把分类器import进来，并训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(vec.transform(x_train), y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "看看我们的准确率如何"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.840860313256313"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.score(vec.transform(x_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21899"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以看到，在2w多个样本上，我们能在5个类别上达到84的准确率。\n",
    "\n",
    "有没有办法提高一些准确率呢？\n",
    "\n",
    "我们可以把特征做得更棒一点，<br>\n",
    "比如，试试加入抽取2-gram和3-gram的统计特征；<br>\n",
    "比如，可以把词库的量放大一点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vec = CountVectorizer(\n",
    "    analyzer='word', # tokenise by character ngrams 解析器：基于空格来区分不同的词\n",
    "    ngram_range=(1,4), # use ngrams of size 1、2、3 保留单个词，两两词、三三词\n",
    "    max_features=20000, # keep the most common 20000 ngrams 保留最高频的20000个词\n",
    ")\n",
    "vec.fit(x_train)\n",
    "\n",
    "def get_features(x):\n",
    "    vec.transform(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分类训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8807251472669985"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(vec.transform(x_train), y_train)\n",
    "classifier.score(vec.transform(x_test), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 交叉验证"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "更可靠的验证效果的方式是交叉验证，但是交叉验证最好保证每一份里面的样本类别也是相对均衡的，我们这里使用StratifiedKFold（k折交叉验证）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8806864356231184\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, precision_score\n",
    "import numpy as np\n",
    "\n",
    "# 分层抽样，保证样本的均衡性\n",
    "def stratifiedkfold_cv(x, y, clf_class, shuffle=True, n_fold=5, **kwargs):\n",
    "    stratifiedk_fold = StratifiedKFold(y, n_folds=n_fold, shuffle=shuffle)\n",
    "    y_pred = y[:]\n",
    "    for train_index, test_index in stratifiedk_fold:\n",
    "        X_train, X_test = x[train_index], x[test_index]\n",
    "        y_train = y[train_index]\n",
    "        clf = clf_class(**kwargs)\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred[test_index] = clf.predict(X_test)\n",
    "    return y_pred\n",
    "\n",
    "NB = MultinomialNB\n",
    "print(precision_score(y, stratifiedkfold_cv(vec.transform(x), np.array(y), NB), average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们做完K折的交叉验证，可以看到在5个类别上的结果平均准确度约为88%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 我们自己来完成一个文本分类器class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "\n",
    "class TextClassifier():\n",
    "\n",
    "    def __init__(self, classifier=MultinomialNB()):\n",
    "        self.classifier = classifier\n",
    "        self.vectorizer = CountVectorizer(analyzer='word', ngram_range=(1,4), max_features=20000)\n",
    "\n",
    "    def features(self, X):\n",
    "        return self.vectorizer.transform(X)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.vectorizer.fit(X)\n",
    "        self.classifier.fit(self.features(X), y)\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.classifier.predict(self.features([x]))\n",
    "\n",
    "    def score(self, X, y):\n",
    "        return self.classifier.score(self.features(X), y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['technology']\n",
      "0.8807251472669985\n"
     ]
    }
   ],
   "source": [
    "text_classifier = TextClassifier()\n",
    "text_classifier.fit(x_train, y_train)\n",
    "#print(text_classifier.predict('这 是 有史以来 最 大 的 一 次 军舰 演习'))\n",
    "print(text_classifier.predict('苹果 公司 有 新 的 发布 计划'))\n",
    "print(text_classifier.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM文本分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们来试试支持向量机的作用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8464770080825609"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "svm = SVC(kernel='linear')\n",
    "svm.fit(vec.transform(x_train), y_train)\n",
    "svm.score(vec.transform(x_test), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以试试rbf核"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.512306498013608"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "svm = SVC(kernel='rbf')\n",
    "svm.fit(vec.transform(x_train), y_train)\n",
    "svm.score(vec.transform(x_test), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 换特征/模型试试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "class TextClassifier():\n",
    "\n",
    "    def __init__(self, classifier=SVC(kernel='linear')):\n",
    "        self.classifier = classifier\n",
    "        self.vectorizer = TfidfVectorizer(analyzer='word', ngram_range=(1,3), max_features=12000)\n",
    "\n",
    "    def features(self, X):\n",
    "        return self.vectorizer.transform(X)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.vectorizer.fit(X)\n",
    "        self.classifier.fit(self.features(X), y)\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.classifier.predict(self.features([x]))\n",
    "\n",
    "    def score(self, X, y):\n",
    "        return self.classifier.score(self.features(X), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['military']\n",
      "0.8778026393899265\n"
     ]
    }
   ],
   "source": [
    "text_classifier = TextClassifier()\n",
    "text_classifier.fit(x_train, y_train)\n",
    "print(text_classifier.predict('这 是 有史以来 最 大 的 一 次 军舰 演习'))\n",
    "print(text_classifier.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
